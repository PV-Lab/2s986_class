{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Property Prediction and Inverse Design\n",
    "\n",
    "---\n",
    "\n",
    "## Hands-on demo\n",
    "\n",
    "---\n",
    "\n",
    "### Goals:\n",
    "* Visualize and manipulate data using `pandas`\n",
    "* Fit and tune models using `sklearn`\n",
    "* Inverse design using `pyswarm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Module imports and global options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils  # we define some useful shortcuts here\n",
    "\n",
    "np.random.seed(0)\n",
    "pd.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 0. Importing the dataset\n",
    "\n",
    "We use the `pandas` library built on top of `numpy` for easy import and manipulation of datasets from a variety of formats (excel, csv, etc). Only the simplest functions are used here. Complete documentation is found [here](https://pandas.pydata.org/).\n",
    "\n",
    "## Dataset\n",
    "\n",
    "[Concrete Compressive Strength Data Set](<http://archive.ics.uci.edu/ml/datasets/concrete+compressive+strength>)\n",
    "\n",
    "The input values $X$ are columns 1-8, representing the various compositions of concrete. The target values $y$ are the compressive strengths in the last column, which is a function of the input compositions.\n",
    "\n",
    "$$ y = f(X) $$\n",
    "\n",
    "Our goal is to **approximate** this function $f(.)$ by some function $\\hat{f}(\\cdot,\\theta)$, and then learn $\\theta$ using data.\n",
    "\n",
    "\n",
    "### Note\n",
    "\n",
    "More datasets can be found at [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./data/Concrete_Data.xls', sheet_name='Sheet1')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Train-test split\n",
    "\n",
    "Let us now split the dataset into training and test sets. We use the `train_test_split()` function from `sklearn.model_selection`, where we simply need to specify the ratio of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = df[df.columns[:-1]], df[df.columns[-1]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Regression Baseline\n",
    "\n",
    "We first consider a linear regression baseline, where we fit a linear model\n",
    "\n",
    "$$ \\hat{f}(X,M,c) = X M + c $$ and then minimize the $L^2$ error\n",
    "\n",
    "$$ \\min_{M,c} \\frac{1}{2} \\Vert y - X M - c \\Vert^2 $$\n",
    "\n",
    "To do this we simply import the function from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_hat_train = regressor.predict(X_train)  # Training set predictions\n",
    "y_hat_test = regressor.predict(X_test)  # Test set predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predictions\n",
    "\n",
    "Hereafter, we use simple functions defined in `utils` to do the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_predictions(\n",
    "    y=[y_train, y_test], y_hat=[y_hat_train, y_hat_test],\n",
    "    labels=['Train', 'Test']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gradient Boosting Regression\n",
    "\n",
    "Let us now use a more robust regressor for non-linear regression. Again, we use canned implementations from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "regressor = GradientBoostingRegressor()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_hat_train = regressor.predict(X_train)\n",
    "y_hat_test = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_predictions(\n",
    "    y=[y_train, y_test], y_hat=[y_hat_train, y_hat_test],\n",
    "    labels=['Train', 'Test']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "\n",
    "Decision tree based ensemble models can also tell us how sensitive (in a very loose sense) the output is to each input parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_feature_importances(\n",
    "        importances=regressor.feature_importances_,\n",
    "        columns=df.columns[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Overfitting\n",
    "\n",
    "Here, let us demonstrate overfitting for the gradient boosting regressor. This can be done by drastically increasing the model complexity. One simple way to increase the model complexity is by increasing the `max_depth` parameter in `GradientBoostingRegressor()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "regressor = GradientBoostingRegressor(max_depth=10)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_hat_train = regressor.predict(X_train)\n",
    "y_hat_test = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predictions\n",
    "\n",
    "Notice that although the training error has decreased drastically, the test error actually got a little worse. This is a classic case of *over-fitting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_predictions(\n",
    "    y=[y_train, y_test], y_hat=[y_hat_train, y_hat_test],\n",
    "    labels=['Train', 'Test']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Hyper-parameter Tuning\n",
    "\n",
    "Observe that `GradientBoostingRegressor()` performed much better than `LinearRegression()`. Can we improve it further?\n",
    "\n",
    "Let us take a quick look at the documentation of `GradientBoostingRegressor()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(GradientBoostingRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning via Random Search CV\n",
    "\n",
    "As seen above, there are many parameters one can tune (e.g. learning_rate, n_estimators etc.). We call these *hyper-parameters*, in the sense that they are parameters controlling the properties of the regressor, and are not the *trainable* parameters during model fitting. \n",
    "\n",
    "To maximize performance, we have to tune these parameters. To do this, we use *random search cross-validation tuning*. Let us briefly explain each term\n",
    "\n",
    "1. Cross-validation: This refers to scoring the performance of a model under a set of hyper-parameters given the training set. The idea is to further split the training set into two\n",
    "    * train set (to be used for training, e.g. 2/3 of original training data)\n",
    "    * validation set (to be used for evaluation of accuracy, 1/3 of original training data)\n",
    "\n",
    "By averaging over the 3 possible splits, we can have an average score of this particular selection of hyper-parameters. The goal is maximize this score over the hyper-parameter space. This is called *3-fold* cross-validation.\n",
    "\n",
    "1. Random search: Instead of performing a grid search over all hyper-parameters, it is usually more efficient to randomly sample them from some distributions and at each CV run, we pick a random hyper-parameter combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define search space\n",
    "The `scipy.stats` module allows us to specify probabily distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats \n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': stats.randint(low=10, high=1000),\n",
    "    'max_depth': stats.randint(low=2, high=6),\n",
    "    'min_samples_split': stats.randint(low=2, high=5),\n",
    "    'learning_rate': [1, 0.5, 0.25, 0.1, 0.05, 0.01]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a CV-tuned regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "regressor_cv = RandomizedSearchCV(\n",
    "    regressor, param_distributions=param_distributions,\n",
    "    n_iter=50, verbose=1)\n",
    "regressor_cv.fit(X_train, y_train)\n",
    "\n",
    "print('Best params: \\n', regressor_cv.best_params_)\n",
    "\n",
    "y_hat_train = regressor_cv.predict(X_train)\n",
    "y_hat_test = regressor_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predictions and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions and feature importances\n",
    "utils.plot_predictions(\n",
    "    y=[y_train, y_test],\n",
    "    y_hat=[y_hat_train, y_hat_test],\n",
    "    labels=['Train', 'Test']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_feature_importances(\n",
    "        importances=regressor_cv.best_estimator_.feature_importances_,\n",
    "        columns=df.columns[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inverse Design\n",
    "\n",
    "After fitting our model, we perform inverse design. In this demo, we do this using the `pyswarm` module, which is an implementation of the *particle swarm optimization* method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refit using tuned hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = regressor_cv.best_params_\n",
    "regressor = GradientBoostingRegressor()\n",
    "regressor.set_params(**best_params)\n",
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounds, objectives and constraints\n",
    "Next, we define some bounds, objectives and constraints to be used for inverse design.\n",
    "\n",
    "1. Upper bounds for all compositions is $1.5\\times$ the 75th percentile of the training data.\n",
    "1. Lower bounds for all compositions is $0.5\\times$ the 25th percentile of the training data\n",
    "1. Objective: minimize *Blast Furnace Slag, Fly Ash, Superplasticizer* compositions\n",
    "1. Constraints:\n",
    "    * Compressive strength >= 70 MPa\n",
    "    * Water <= 150 kg / m^3\n",
    "    * Age <= 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bounds = np.percentile(X, 75, axis=0) * 1.5\n",
    "lower_bounds = np.percentile(X, 25, axis=0) * 0.5\n",
    "\n",
    "def objective(X):\n",
    "    \"\"\"\n",
    "    We want to minimize\n",
    "        Blast Furnace Slag, Fly Ash, Superplasticizer\n",
    "    \"\"\"\n",
    "    return X[1]**2 + X[2]**2 + X[4]**2\n",
    "\n",
    "def constraints(X):\n",
    "    \"\"\"\n",
    "    We want to following constraints:\n",
    "        1. Compressive strength >= 70 MPa\n",
    "        2. Water <= 150 kg / m^3\n",
    "        3. Age <= 30 days\n",
    "    \"\"\"\n",
    "    predicted_strength = regressor.predict(X.reshape(1, -1))\n",
    "    cons_str_lower = predicted_strength - 70\n",
    "    cons_water_upper = 150 - X[3]\n",
    "    cons_age_upper = 30 - X[-1]\n",
    "    return [cons_str_lower, cons_water_upper, cons_age_upper]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design via particle swarm optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyswarm import pso\n",
    "\n",
    "X_opts = []\n",
    "n_runs = 5\n",
    "for n in range(n_runs):\n",
    "    X_opt, _ = pso(\n",
    "        objective, lower_bounds, upper_bounds, f_ieqcons=constraints,\n",
    "        swarmsize=100, maxiter=200)\n",
    "    X_opts.append(X_opt)\n",
    "X_opts = np.asarray(X_opts)\n",
    "y_hat_opts = regressor.predict(X_opts).reshape(-1, 1)\n",
    "data_opt = np.concatenate([X_opts, y_hat_opts], axis=1)\n",
    "df_predict = pd.DataFrame(columns=df.columns, data=data_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with unseen data\n",
    "\n",
    "In fact, our dataset used in this demo is not the full dataset. We took out one sample that satisfies the constraints above and minimizes the objective. Let us now check how close our inverse-design results are to this unseen data point (colored red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unseen = pd.read_excel('./data/Concrete_Data_unseen.xls', sheet_name='Sheet1')\n",
    "df_combined = pd.concat([df_predict, df_unseen], ignore_index=True)\n",
    "df_combined.style.applymap(lambda x: 'color: red', subset=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
