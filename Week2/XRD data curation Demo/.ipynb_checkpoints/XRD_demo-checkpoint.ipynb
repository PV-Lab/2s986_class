{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# small dataset XRD classification using machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This is meant to be a (relatively) self-contained example of XRD classification on small dataset via physics based data augmentation\n",
    "The overall procedure is:\n",
    "   1. Load the experimental and theoretical XRD spectra with dimensionality labels\n",
    "   2. Data preprocessing for experimental data\n",
    "   3. Data augmentation for both experimental and theoretical spectra based on the characteristics of thin film XRD measurement\n",
    "   4. Perform dimensionality/space group classification based on the post-processed data\n",
    "   5. Cross validation and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preload libraries and functions\n",
    "First of all, let's import libraries that will be used in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  \n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.signal import find_peaks_cwt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import multiple classification algorithms from scikt-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes Classifier\n",
    "def naive_bayes_classifier(train_x, train_y):\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    model = MultinomialNB(alpha=0.01)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "\n",
    "# KNN Classifier\n",
    "def knn_classifier(train_x, train_y):\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    model = KNeighborsClassifier(n_neighbors=3)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Logistic Regression Classifier\n",
    "def logistic_regression_classifier(train_x, train_y):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    model = LogisticRegression(penalty='l2')\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Random Forest Classifier\n",
    "def random_forest_classifier(train_x, train_y):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    model = RandomForestClassifier(n_estimators=100)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Decision Tree Classifier\n",
    "def decision_tree_classifier(train_x, train_y):\n",
    "    from sklearn import tree\n",
    "    model = tree.DecisionTreeClassifier()\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "\n",
    "# GBDT(Gradient Boosting Decision Tree) Classifier\n",
    "def gradient_boosting_classifier(train_x, train_y):\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    model = GradientBoostingClassifier(n_estimators=100)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "\n",
    "# SVM Classifier\n",
    "def svm_classifier(train_x, train_y):\n",
    "    from sklearn.svm import SVC\n",
    "    model = SVC(kernel='rbf', probability=True)\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    " # 3 layer neural network classficiation\n",
    "def mlp_classifier(train_x,train_y):\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    model =  MLPClassifier(hidden_layer_sizes=(256,256,256), max_iter=200, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-6, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "    model.fit(train_x,train_y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create functions that can read .ASC and .xy files from subfolders(this function is not used in the demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets .ASC files from directory\n",
    "def spectra_list(path,excluded):\n",
    "    file_pth= [os.path.join(d, x) for d, dirs, files in os.walk(path) for x in files if x.endswith(\".ASC\") and excluded not in x]\n",
    "    return file_pth\n",
    "#Gets .XY files from directory\n",
    "def spectra_list2(path):\n",
    "    file_pth= [os.path.join(d, x) for d, dirs, files in os.walk(path) for x in files if x.endswith(\".xy\")]\n",
    "    return file_pth\n",
    "#Groups all curves within a symmetry group into as single dataframe\n",
    "def group(spectra,k):\n",
    "    groups=[]\n",
    "    for indx,vals in enumerate(spectra[k]):\n",
    "        groups.append(pd.read_csv(spectra[k][indx], delim_whitespace=True, header=None))\n",
    "        df=pd.concat(groups, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the XRD intensities are arbitrary units, we will scale the XRD intensities from 0 to 1 for ML input. Let's define functions that normalize the data from 0 to 1 based on the data structure. You can use min_max scaler from SK learn but since the data structure is not standardized. We fine our own min_max scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data normalization from 0 to 1 for double column dataframe\n",
    "def normdata(data):\n",
    "    (len1,w1) = np.shape(data)\n",
    "    ndata = np.zeros([len1,w1//2])\n",
    "    for i in range(w1//2):\n",
    "        ndata[:,i]=(data[:,2*i+1]-min(data[:,2*i+1]))/(max(data[:,2*i+1])-min(data[:,2*i+1]))\n",
    "    return ndata\n",
    "#data normalization from 0 to 1 for single column dataframe\n",
    "def normdatasingle(data):\n",
    "    (len1,w1) = np.shape(data)\n",
    "    ndata = np.zeros([len1,w1])\n",
    "    for i in range(w1):\n",
    "        ndata[:,i]=(data[:,i]-min(data[:,i]))/(max(data[:,i])-min(data[:,i]))\n",
    "    return ndata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have ~200 spectra in our subfolders, let's define data augmentation functions based on our domain knowledge on thin-film and power XRD spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data augmendatation for simulated XRD spectrum\n",
    "def augdata(data,num,dframe,minn,maxn,labels):\n",
    "    np.random.seed(1234)\n",
    "    (len1,w1) = np.shape(data)\n",
    "    augd =np.zeros([len1,num])\n",
    "    naugd=np.zeros([len1,num])\n",
    "    newaugd=np.zeros([len1,num])\n",
    "    crop_augd = np.zeros([maxn-minn,num])\n",
    "    par1 = labels\n",
    "    pard = []\n",
    "    for i in range(num):\n",
    "        rnd = np.random.randint(0,w1)\n",
    "        # create the first filter for peak elimination\n",
    "        dumb= np.repeat(np.random.choice([0,1,1],300),len1//300)\n",
    "        dumb1= np.append(dumb,np.zeros([len1-len(dumb),]))\n",
    "        # create the second filter for peak scaling\n",
    "        dumbrnd= np.repeat(np.random.rand(100,),len1//100)\n",
    "        dumbrnd1=np.append(dumbrnd,np.zeros([len1-len(dumbrnd),]))\n",
    "        #peak eleminsation and scaling\n",
    "        augd[:,i] = np.multiply((data[:,rnd]),dumbrnd1)\n",
    "        augd[:,i] = np.multiply(augd[:,i],dumb1)\n",
    "        #nomrlization\n",
    "        naugd[:,i] = (augd[:,i]-min(augd[:,i]))/(max(augd[:,i])-min(augd[:,i])+1e-9)\n",
    "        pard.append (par1[2*rnd])\n",
    "        #adding shift\n",
    "        cut = np.random.randint(-20*1,20)\n",
    "        #XRD spectrum shift to left\n",
    "        if cut>=0:\n",
    "            newaugd[:,i] = np.append(naugd[cut:,i],np.zeros([cut,]))\n",
    "        #XRD spectrum shift to right\n",
    "        else:\n",
    "            newaugd[:,i] = np.append(naugd[0:len1+cut,i],np.zeros([cut*-1,]))\n",
    "\n",
    "        crop_augd[:,i] = newaugd[minn:maxn,i]\n",
    "#\n",
    "    return newaugd, pard,crop_augd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data augmentation for experimental XRD spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_augdata(data,num,label):\n",
    "    np.random.seed(1234)\n",
    "    (len1,w1) = np.shape(data)\n",
    "    augd =np.zeros([len1,num])\n",
    "    naugd=np.zeros([len1,num])\n",
    "    newaugd=np.zeros([len1,num])\n",
    "    par=np.zeros([num,])\n",
    "    for i in range(num):\n",
    "        rnd = np.random.randint(0,w1)\n",
    "\n",
    "        # create the first filter for peak elimination\n",
    "        dumb= np.repeat(np.random.choice([0,1,1],300),len1//300)\n",
    "        dumb1= np.append(dumb,np.zeros([len1-len(dumb),]))\n",
    "        # create the second filter for peak scaling\n",
    "        dumbrnd= np.repeat(np.random.rand(200,),len1//200)\n",
    "        dumbrnd1=np.append(dumbrnd,np.zeros([len1-len(dumbrnd),]))\n",
    "        #peak eleminsation and scaling\n",
    "        augd[:,i] = np.multiply((data[:,rnd]),dumbrnd1)\n",
    "        augd[:,i] = np.multiply(augd[:,i],dumb1)\n",
    "        #nomrlization\n",
    "        naugd[:,i] = (augd[:,i]-min(augd[:,i]))/(max(augd[:,i])-min(augd[:,i])+1e-9)\n",
    "        par[i,] =label[rnd,]\n",
    "        #adding shift\n",
    "        cut = np.random.randint(-20*1,20)\n",
    "        #XRD spectrum shift to left\n",
    "        if cut>=0:\n",
    "            newaugd[:,i] = np.append(naugd[cut:,i],np.zeros([cut,]))\n",
    "        #XRD spectrum shift to right\n",
    "        else:\n",
    "            newaugd[:,i] = np.append(naugd[0:len1+cut,i],np.zeros([cut*-1,]))\n",
    "\n",
    "    return newaugd, par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experimental data contains noise and background. Let's write a function to remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting exprimental data\n",
    "def exp_data_processing (data,minn,maxn,window):\n",
    "    (len1,w1) = np.shape(data)\n",
    "    nexp1 =np.zeros([maxn-minn,w1])\n",
    "    for i in range(w1):\n",
    "        #savgol_filter to smooth the data\n",
    "         new1 = savgol_filter(data[minn:maxn,i], 31, 3)\n",
    "         #peak finding\n",
    "         zf= find_peaks_cwt(new1, np.arange(10,15), noise_perc=0.01)\n",
    "         #background substraction\n",
    "         for j in range(len(zf)-1):\n",
    "             zf_start= np.maximum(0,zf[j+1]-window//2)\n",
    "             zf_end = np.minimum(zf[j+1]+window//2,maxn)\n",
    "             peak = new1[zf_start:zf_end]\n",
    "\n",
    "             ##abritaryly remove 1/4 data\n",
    "             npeak = np.maximum(0,peak-max(np.partition(peak,window//5 )[0:window//5]))\n",
    "             nexp1[zf_start:zf_end,i]= npeak\n",
    "    return nexp1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data\n",
    "The XRD spectra contain both experimental and theoretical data. The theoretical spectra are power XRD spectra. The specific compound formulae of each XRD spectrum is scrubbed for data privacy issues; keep an eye open for our upcoming NIPS and arXiv publications for labeled datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load simulated XRD spectra\n",
    "\n",
    "theor=pd.read_csv('theor.csv',header=None)\n",
    "\n",
    "# Load meaured XRD spectra\n",
    "\n",
    "exp=pd.read_csv('exp.csv',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theor.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert this big matrix from string to number and take out the first row as \"labels\" for our machine learning problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label\n",
    "theor_labels= theor.iloc[0]\n",
    "#take out the first row\n",
    "theor = theor.drop(theor.index[0])\n",
    "#convert from string to number\n",
    "theor = theor.apply(pd.to_numeric, errors='coerce')\n",
    "#convert from pandas dataframe to numpy array\n",
    "theor_arr=theor.as_matrix()\n",
    "#normalization\n",
    "ntheor = normdata (theor_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the theoretical spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(theor_arr[:,0],theor_arr[:,1],label='Theorectical')\n",
    "plt.xlabel('2theta angle[degrees]')\n",
    "plt.ylabel('Intensity [a.u.]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(theor_arr[:,0],ntheor[:,0],label='Theorectical')\n",
    "plt.xlabel('2theta angle[degrees]')\n",
    "plt.ylabel('Normalized Intensity [a.u.]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is size of our theoretical XRD spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntheor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the experimental spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the same operation as what we have done for the theoretical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels\n",
    "exp_labels= exp.iloc[0]\n",
    "#take out the first row\n",
    "exp = exp.drop(exp.index[0])\n",
    "#string to number\n",
    "exp=exp.apply(pd.to_numeric, errors='coerce')\n",
    "#dataframe to array\n",
    "exp_arr=exp.as_matrix()\n",
    "\n",
    "#We didn't simulate the peak at 5.00 degrees, so start from 5.04\n",
    "exp_arr=exp_arr[1:,:]\n",
    "\n",
    "#normalization\n",
    "ntheor = normdata (theor_arr)\n",
    "nexp = normdata (exp_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the shape of this matrix after normalization? (in other words, what are the available experimental data size?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nexp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing\n",
    "### Trim the data\n",
    "Since not all the data has the same range(2theta angles), we need to unify the range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the range for spectrum (this is to unify the measurement range)\n",
    "exp_min = 0\n",
    "exp_max = 1350\n",
    "theor_min = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot  the measured spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(exp_arr[exp_min:exp_max,0],exp_arr[exp_min:exp_max,3],label='Experimental data')\n",
    "plt.xlabel('2theta angle[degrees]')\n",
    "plt.ylabel('Intensity [a.u.]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background and noise subtraction\n",
    "window size is a hyperparamtere that we can change to determine the width of peaks. We call a function which is previously defined(exp_data_processing) to remove the measurement noise and signals from the substrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#window size for experimental data extraction\n",
    "window =15\n",
    "theor_max = theor_min+exp_max-exp_min\n",
    "#experimetal data input\n",
    "post_exp= normdatasingle(exp_data_processing (nexp,exp_min,exp_max,window))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the experimental spectra again after data post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig ,ax1 = plt.subplots()\n",
    "ax1.plot(exp_arr[exp_min:exp_max,0],exp_arr[exp_min:exp_max,3])\n",
    "ax1.set_xlabel('2theta angle[degrees]')\n",
    "ax1.set_ylabel('Intensity [a.u.]')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(exp_arr[exp_min:exp_max,0],post_exp[:,1],color ='r')\n",
    "ax2.set_ylabel('Normalized Intensity [a.u.]')\n",
    "fig.tight_layout()\n",
    "plt.legend(['Post processing'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data augmentation \n",
    "\n",
    "Let's augment the data for the theoretical dataset first\n",
    "\n",
    "Specify how many data points we augmented for theoretical and experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's start to do the data augmentation.\n",
    "theor_aug_num = 1000\n",
    "exp_aug_num = 1000\n",
    "augd,pard,crop_augd = augdata(ntheor,theor_aug_num,theor,theor_min,theor_max,theor_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start from one theoretical spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = 0#np.random.randint(0,100)\n",
    "demo_t = ntheor[theor_min:theor_max,(rnd)]\n",
    "demo_x = theor_arr[theor_min:theor_max,0]\n",
    "plt.plot(demo_x,demo_t,label='Original')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(demo_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some peaks will not be visible due to preferred orientation, crystal size etc. We will add a periodic blocking filter which randomly eliminates peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add in the first filter (peak elimination)\n",
    "dum1= np.repeat(np.random.choice([0,0,1],270),len(demo_x)//270)\n",
    "demo_1st = np.multiply( demo_t,dum1)\n",
    "#plot \n",
    "plt.plot(demo_x,demo_1st,label='Peak Elimination', color= 'r')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative intensities of those peaks also depends on the  preferred orientation . We will add another periodic filter that scales intensities randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum2= np.repeat(np.random.rand(135,),len(demo_x)//135)\n",
    "demo_2nd = np.multiply( demo_1st,dum2)\n",
    "#plot \n",
    "plt.plot(demo_x,demo_2nd,label='Peak scaling', color= 'k')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the strain and instrumental error may cause the shift of the spectra. We will shift the spectra within a limited range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = np.random.randint(-20*1,20)\n",
    "#XRD spectrum shift to left\n",
    "if cut>=0:\n",
    "    demo_3rd = np.append(demo_2nd[cut:,],np.zeros([cut,]))\n",
    "#XRD spectrum shift to right\n",
    "else:\n",
    "    demo_3rd = np.append(demo_2nd[0:len(demo_x)+cut,],np.zeros([cut*-1,]))\n",
    "#plot \n",
    "plt.plot(demo_x,demo_2nd,label='Peak shift', color= 'b')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()   \n",
    "           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat this augmentation process many times for all spectra.\n",
    "\n",
    "Now we  will augment the spectra both for experiment and theory from ~200 to 2000!\n",
    "\n",
    "We should add labels to those augmented spectra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert theorectical label from dimensionality to numbers\n",
    "directory = ['0','2','3']\n",
    "label_t=np.zeros([len(pard),])\n",
    "for i in range(len(pard)): \n",
    "    temp = pard[i]\n",
    "    label_t[i]=directory.index(temp[0])\n",
    "\n",
    "#convert experimental label from dimensionality to numbers\n",
    "\n",
    "par_exp = exp_labels\n",
    "\n",
    "label_exp=np.zeros([len(par_exp)//2,])\n",
    "\n",
    "for i in range(len(par_exp)//2):\n",
    "\n",
    "    temp = par_exp[2*i]\n",
    "    label_exp[i]=directory.index(temp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Classification and cross validation using various ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After data extraction, data preprocessing and data augmentation. We have ~2000 spectra now as an Input to train our machine learning algorithm. We can use part of those spectra to fine tune these hyperparameters and test the \"untouched\" spectra . The test  was done in the paper and will not be conducted here for the sake of time\n",
    "Let's determine how many spectra we want to use to do a cross validation for our machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the number of experimetal spectra we use for a N flold cross validation\n",
    "exp_num =60  \n",
    "X_exp = np.transpose(post_exp[:,0:exp_num])\n",
    "y_exp = label_exp[0:exp_num]\n",
    "#train and test split for the experimental data\n",
    "#X_train_exp, X_test_exp, y_train_exp, y_test_exp = train_test_split(X_exp\n",
    "      #  ,y_exp , test_size=0.33,random_state=1)\n",
    "\n",
    "\n",
    "\n",
    "#train and test split for the theorectical data\n",
    "X_th = np.transpose(crop_augd )\n",
    "y_th = label_t\n",
    "#X_train_th, X_test_th, y_train_th, y_test_th = train_test_split( \n",
    "       # X_th, y_th, test_size=0.33,random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data to N folds ---(N-1)/N% training, 1/N% test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "from sklearn.model_selection import KFold\n",
    "k_fold = KFold(n_splits=n_fold, shuffle=True,random_state=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose your favorate machine learning algorithem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier = ['RF']\n",
    "classifiers = {'NB':naive_bayes_classifier,   \n",
    "                   'KNN' :knn_classifier, \n",
    "                   'LR':logistic_regression_classifier,  \n",
    "                   'RF':random_forest_classifier,  \n",
    "                   'DT':decision_tree_classifier,  \n",
    "                   'SVM':svm_classifier,                    \n",
    "                   'GBDT':gradient_boosting_classifier,\n",
    "                   'NN':mlp_classifier\n",
    "    }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_exp = np.empty((n_fold,1)) \n",
    "start_time = time.time()   \n",
    "for classifier in test_classifier: \n",
    "    print ('******************* %s ********************' % test_classifier)\n",
    "    for k, (train, test) in enumerate(k_fold.split(X_exp, y_exp)):       \n",
    "           \n",
    "            #data augmentation to experimenal traning dataset\n",
    "            temp_x = X_exp[train]\n",
    "            temp_y = y_exp[train]\n",
    "            exp_train_x,exp_train_y = exp_augdata(temp_x.T,exp_aug_num ,temp_y)\n",
    "            #combine theorectical and experimenal dataset for training\n",
    "            train_combine = np.concatenate((X_th,exp_train_x.T))\n",
    "            train_y = np.concatenate((y_th,exp_train_y))\n",
    "             \n",
    "            model = classifiers[classifier](train_combine, train_y)  \n",
    "    \n",
    "            #predict experimental prediction accuracy\n",
    "            predict_exp = model.predict(X_exp[test])        \n",
    "            accuracy_exp[k] = accuracy_score(y_exp[test], predict_exp)  \n",
    "            print ('accuracy_exp: %.2f%%' % (100 * accuracy_exp[k]))\n",
    "            # 5 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print ('CV  took %fs!' % (time.time() - start_time) )\n",
    "print('Cross-validation results:')\n",
    "print('Folds: %i, mean acc: %.3f' % (len(accuracy_exp), np.mean(np.abs(accuracy_exp))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Hyper parameter tunning \n",
    "\n",
    "We can fine tune the hyperparameters in both the classifers and data preproessing&augmentation for each method we tried. We find a three layer DNN performs better than other classifiers for both cross validation and newly synthezied validation dataset."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
